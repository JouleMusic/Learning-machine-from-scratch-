
# One-Hot Encoding

## 简介
又叫读热数据编码

One-Hot编码，又称为一位有效编码，主要是采用位状态寄存器来对个状态进行编码，每个状态都由他独立的寄存器位，并且在任意时候只有一位有效。


在实际的应用场景中，有非常多的特征不是连续的数值变量，而是某一些离散的类别。比如在广告系统中，用户的性别，用户的地址，用户的兴趣爱好等等一系列特征，都是一些分类值。这些特征一般都无法直接应用在需要进行数值型计算的算法里，比如CTR预估中最常用的LR。那针对这种情况最简单的处理方式是将不同的类别映射为一个整数，比如男性是0号特征，女性为1号特征。这种方式最大的优点就是简单粗暴，实现简单。那最大的问题就是在这种处理方式中，各种类别的特征都被看成是有序的，这显然是非常不符合实际场景的。

为了解决上述问题，其中一种可能的解决方法是采用独热编码（One-Hot Encoding）。
独热编码即 One-Hot 编码，又称一位有效编码，其方法是使用N位状态寄存器来对N个状态进行编码，每个状态都由他独立的寄存器位，并且在任意时候，其中只有一位有效。可以这样理解，对于每一个特征，如果它有m个可能值，那么经过独热编码后，就变成了m个二元特征。并且，这些特征互斥，每次只有一个激活。因此，数据会变成稀疏的。（本段内容来自网络）

## 为什么能使用one hot

1. 使用one-hot编码，将离散特征的取值扩展到了欧式空间，离散特征的某个取值就对应欧式空间的某个点。

2. 将离散特征通过one-hot编码映射到欧式空间，是因为，在回归，分类，聚类等机器学习算法中，特征之间距离的计算或相似度的计算是非常重要的，而我们常用的距离或相似度的计算都是在欧式空间的相似度计算，计算余弦相似性，基于的就是欧式空间。

3. 将离散型特征使用one-hot编码，可以会让特征之间的距离计算更加合理。比如，有一个离散型特征，代表工作类型，该离散型特征，共有三个取值，不使用one-hot编码，其表示分别是x_1 = (1), x_2 = (2), x_3 = (3)。两个工作之间的距离是，(x_1, x_2) = 1, d(x_2, x_3) = 1, d(x_1, x_3) = 2。那么x_1和x_3工作之间就越不相似吗？显然这样的表示，计算出来的特征的距离是不合理。那如果使用one-hot编码，则得到x_1 = (1, 0, 0), x_2 = (0, 1, 0), x_3 = (0, 0, 1)，那么两个工作之间的距离就都是sqrt(2).即每两个工作之间的距离是一样的，显得更合理。


## One-Hot Encoding的处理方法
对于上述的问题，性别的属性是二维的，同理，地区是三维的，浏览器则是思维的，这样，我们可以采用One-Hot编码的方式对上述的样本“["male"，"US"，"Internet Explorer"]”编码，“male”则对应着[1，0]，同理“US”对应着[0，1，0]，“Internet Explorer”对应着[0,0,0,1]。则完整的特征数字化的结果为：[1,0,0,1,0,0,0,0,1]。这样导致的一个结果就是数据会变得非常的稀疏。

可以这样理解，对于每一个特征，如果它有m个可能值，那么经过独热编码后，就变成了m个二元特征。并且，这些特征互斥，每次只有一个激活。因此，数据会变成稀疏的。

**<font color=red>这样做的好处主要有：</font>**<br>

* 解决了分类器不好处理属性数据的问题
* 在一定程度上也起到了扩充特征的作用

这篇blog写的挺好，我就直接拿过来粘上了，[出自](http://blog.csdn.net/google19890102/article/details/44039761)


## sklearn one hot encoder

sklearn.preprocessing.OneHotEncoder

one hot encoder 不仅对 label 可以进行编码，还可对 categorical feature 进行编码：

```
>>> from sklearn.preprocessing import OneHotEncoder
>>> enc = OneHotEncoder()

>>> enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])  

>>> enc.n_values_
array([2, 3, 4])

>>> enc.feature_indices_
array([0, 2, 5, 9])

>>> enc.transform([[0, 1, 1]]).toarray()
array([[ 1.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.]])


```


为 OneHotEncoder 类传递进来的数据集：

[[0, 0, 3],
[1, 1, 0],
[0, 2, 1],
[1, 0, 2]]

每一列代表一个属性，fit 操作之后：

* 对象enc的n_values_成员变量，记录着每一个属性的最大取值数目，如本例第一个属性：0, 1, 0, 1 ⇒ 2，0, 1, 2, 0 ⇒ 3，3, 0, 1, 2 ⇒ 4； 即各个属性（feature）在 one hot 编码下占据的位数；
* 对象 enc 的 feature_indices_，则记录着属性在新 One hot 编码下的索引位置，
feature_indices_ 是对 n_values_ 的累积值，不过 feature_indices 的首位是 0；

进一步通过 fit 好的 one hot encoder 对新来的特征向量进行编码：

```
>>> enc.transform([[0, 1, 1]]).toarray()
array([[ 1.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.]])
```

* 前 2 位 1, 0，对 0 进行编码
* 中间 3 位 0, 1, 0 对 1 进行编码；
* 末尾 4 位 0, 1, 0, 0 对 1 进行编码；


# pandas get_dummy
另一种常用于统计建模或机器学习的转换方式是：将分类变量（categorical variable）转换为“哑变量矩阵”（dummy matrix）或“指标矩阵”（indicator matrix）。如果DataFrame的某一列中含有k个不同的值，则可以派生出一个k列矩阵或DataFrame（其值全为1和0）。pandas有一个get_dummies函数可以实现该功能（其实自己动手做一个也不难）。拿之前的一个例子来说：([本段转自](http://blog.csdn.net/eshaoliu/article/details/53557989))

```
In [72]: df = pd.DataFrame({'key': ['b', 'b', 'a', 'c', 'a', 'b'],
   ....:                    'data1': range(6)})

In [73]: pd.get_dummies(df['key'])
Out[73]:
   a  b  c
0  0  1  0
1  0  1  0
2  1  0  0
3  0  0  1
4  1  0  0
5  0  1  0

[6 rows x 3 columns]
```
